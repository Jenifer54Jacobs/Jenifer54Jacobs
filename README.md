## Hi there ðŸ‘‹

Welcome to my GitHub! Iâ€™m a data engineer currently focused on learning Apache Spark for large-scale data processing. This repository documents my hands-on journey into distributed computing and big data engineering, where Iâ€™m building skills to handle, transform, and analyze massive datasets efficiently.

Apache Spark has become a core part of modern data engineering, and Iâ€™m excited to dive into its ecosystem. Iâ€™m exploring its fundamental conceptsâ€”like RDDs, DataFrames, and Spark SQLâ€”while also learning how to write scalable, fault-tolerant processing pipelines. My focus is on understanding how to optimize performance, manage memory, and work with large datasets in a distributed environment.

I use PySpark as my main interface with Spark, which allows me to combine the scalability of Spark with the flexibility and readability of Python. Through various projects, Iâ€™m practicing techniques such as ETL (Extract, Transform, Load), data cleaning, aggregations, joins, and writing outputs to different storage formats like Parquet and CSV.

This repository contains learning exercises, project notebooks, and code samples built on public or simulated large datasets. Each project is designed to reinforce a conceptâ€”whether it's filtering billions of records, handling schema inference, or managing partitioning for faster queries.

As I grow in this field, I aim to write clean, efficient, and production-ready code that reflects good engineering practices. If youâ€™re also exploring Spark or working in the data engineering space, feel free to browse the code, share feedback, or collaborate.

Thanks for visitingâ€”and hereâ€™s to mastering big data, one Spark job at a time.
